properties([
    // Job parameter defintions.
    parameters([
        choiceParam(
            description: 'Release or Dev branch to build',
            name: 'Branch',
            choices: 'branch-3.1\nbranch-3.0\nbranch-2.8\nmaster'
        ),
        choiceParam(
            description: 'Avaliable platforms to build off of',
            name: 'Platform',
            choices: 'hdp3.1.0\nhdp3.1.5'
        ),
        stringParam(
            description: 'Tag to build off of i.e. 3.0.1.1998',
            name: 'Tag',
            defaultValue: '3.0.1.1998'
        ),
        booleanParam(
            defaultValue: false, 
            name: 'Release', 
            description: 'Put this release into S3 or not')
    ])
])

def vault_addr="https://vault.build.splicemachine-dev.io"
def branch = ""
def source_branch = "${Branch}"
def source_folder = "."
def splice_version = "${Tag}"
def env_classifier = "${Platform}"
def hbase_version = ""
def hadoop_version = ""
def kafka_version = ""
def spark_folder = ""
def release_flag = "${Release}"

// Launch the docker container
node('splice-standalone') {
    def artifact_values  = [
        [$class: 'VaultSecret', path: "secret/aws/jenkins/colo_jenkins", secretValues: [
            [$class: 'VaultSecretValue', envVar: 'ARTIFACT_USER', vaultKey: 'user'],
            [$class: 'VaultSecretValue', envVar: 'ARTIFACT_PASSWORD', vaultKey: 'pass']]]
    ]
    def vaultSecrets = [
        [$class: 'VaultSecret', path: "secret/aws/jenkins/splicemachine", secretValues: [
            [$class: 'VaultSecretValue', envVar: 'aws_splice_access', vaultKey: 'access'],
            [$class: 'VaultSecretValue', envVar: 'aws_splice_secret', vaultKey: 'secret']]]
    ]
    def region = "us-east-1"
    stage('Login') {
        wrap([$class: 'VaultBuildWrapper', vaultSecrets: vaultSecrets]) {
        sh "aws configure set aws_access_key_id $aws_splice_access"
        sh "aws configure set aws_secret_access_key $aws_splice_secret"
        sh "aws configure set region $region"
        }
    }

    try {

    notifyBuild('STARTED')
    echo source_branch

    stage('Checkout') {
      // Checkout code from repository
        checkout([  
            $class: 'GitSCM', 
            branches: [[name: splice_version]], 
            doGenerateSubmoduleConfigurations: false, 
            extensions: [[$class: 'RelativeTargetDirectory', relativeTargetDir: 'spliceengine']], 
            submoduleCfg: [], 
            userRemoteConfigs: [[credentialsId: '88647ede-744a-444b-8c08-8313cc137944', url: 'https://github.com/splicemachine/spliceengine.git']]
        ])
        checkout([  
            $class: 'GitSCM', 
            branches: [[name: splice_version]],
            doGenerateSubmoduleConfigurations: false, 
            extensions: [[$class: 'RelativeTargetDirectory', relativeTargetDir: 'spliceengine-ee']], 
            submoduleCfg: [], 
            userRemoteConfigs: [[credentialsId: '88647ede-744a-444b-8c08-8313cc137944', url: 'https://github.com/splicemachine/spliceengine-ee.git']]
        ])
        checkout([  
            $class: 'GitSCM', 
            branches: [[name: 'master']],
            doGenerateSubmoduleConfigurations: false, 
            extensions: [[$class: 'RelativeTargetDirectory', relativeTargetDir: 'splice-machine-spark-connector']], 
            submoduleCfg: [], 
            userRemoteConfigs: [[credentialsId: '88647ede-744a-444b-8c08-8313cc137944', url: 'https://github.com/splicemachine/splice-machine-spark-connector.git']]
        ])
    }
    stage('Build Dependencies') {
        wrap([$class: 'VaultBuildWrapper', vaultSecrets: artifact_values]) {
            dir('spliceengine'){
                def platforms = "$env_classifier"
                sh "mvn -B clean install -DskipTests"
                sh '''
                cp pipelines/spot-bugs/template/settings.xml ~/.m2/settings.xml
                sed  -i "s/REPLACE_USER/$ARTIFACT_USER/" ~/.m2/settings.xml
                sed  -i "s/REPLACE_PASS/$ARTIFACT_PASSWORD/" ~/.m2/settings.xml
                '''
                sh "mvn -B clean install -Pcore,$platforms,ee -DskipTests"
                sh '''
                apt-get autoremove --purge scala -y
                wget www.scala-lang.org/files/archive/scala-2.11.8.deb
                dpkg -i scala-2.11.8.deb
                echo "deb https://dl.bintray.com/sbt/debian /" | tee -a /etc/apt/sources.list.d/sbt.list
                apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823
                apt-get update
                apt-get install sbt -y
                '''
            }
        }
      }

    stage('Build SSDS') {
      dir('splice-machine-spark-connector'){
                if ("$env_classifier" == "cdh5.12.0"){
                    spark_folder = "."
                    hbase_version = "1.2.0-\\\${envClassifier.value}"
                    hadoop_version = "2.6.0-\\\${envClassifier.value}"
                    kafka_version = "0.10.0-kafka-2.1.0"
                    spark_version = "2.2.0.cloudera2"
                } else if ("$env_classifier" == "cdh5.12.2"){
                    spark_folder = "."
                    hbase_version = "1.2.0-\\\${envClassifier.value}"
                    hadoop_version = "2.6.0-\\\${envClassifier.value}"
                    kafka_version = "0.10.0-kafka-2.1.0"
                    spark_version = "2.2.0.cloudera2"
                } else if ("$env_classifier" == "cdh5.12.2-2.3"){
                    env_classifier = "cdh5.12.2"
                    spark_folder = "."
                    hbase_version = "1.2.0-\\\${envClassifier.value}"
                    hadoop_version = "2.6.0-\\\${envClassifier.value}"
                    kafka_version = "0.10.0-kafka-2.1.0"
                    spark_version = "2.3.0.cloudera4"
                } else if ("$env_classifier" == "cdh5.13.3"){
                    spark_folder = "."
                    hbase_version = "1.2.0-\\\${envClassifier.value}"
                    hadoop_version = "2.6.0-\\\${envClassifier.value}"
                    kafka_version = "0.10.0-kafka-2.1.0"
                    spark_version = "2.2.0.cloudera2"
                } else if ("$env_classifier" == "cdh5.14.0"){
                    spark_folder = "."
                    hbase_version = "1.2.0-\\\${envClassifier.value}"
                    hadoop_version = "2.6.0-\\\${envClassifier.value}"
                    kafka_version = "0.10.0-kafka-2.1.0"
                    spark_version = "2.2.0.cloudera2"
                } else if ("$env_classifier" == "cdh5.14.2"){
                    spark_folder = "."
                    hbase_version = "1.2.0-\\\${envClassifier.value}"
                    hadoop_version = "2.6.0-\\\${envClassifier.value}"
                    kafka_version = "0.10.0-kafka-2.1.0"
                    spark_version = "2.2.0.cloudera2"
                } else if ("$env_classifier" == "cdh5.16.1"){
                    spark_folder = "."
                    hbase_version = "1.2.0-\\\${envClassifier.value}"
                    hadoop_version = "2.6.0-\\\${envClassifier.value}"
                    kafka_version = "0.10.0-kafka-2.1.0"
                    spark_version = "2.2.0.cloudera2"
                } else if ("$env_classifier" == "cdh6.3.0"){
                    spark_folder = "spark2.4"
                    hbase_version = "2.1.0-\\\${envClassifier.value}"
                    hadoop_version = "3.0.0-\\\${envClassifier.value}"
                    kafka_version = "2.2.1-\\\${envClassifier.value}"
                    spark_version = "2.4.0-\\\${envClassifier.value}"
                } else if ("$env_classifier" == "hdp2.6.1"){
                    spark_folder = "."
                    hbase_version = "1.1.2.2.6.1.0-129"
                    hadoop_version = "2.7.3.2.6.1.0-129"
                    kafka_version = "0.10.1.2.6.1.0-129"
                    spark_version = "2.1.1.2.6.1.0-129"
                } else if ("$env_classifier" == "hdp2.6.3"){
                    spark_folder = "."
                    hbase_version = "1.1.2.2.6.3.0-235"
                    hadoop_version = "2.7.3.2.6.3.0-235"
                    kafka_version = "0.10.1.2.6.3.0-235"
                    spark_version = "2.2.0.2.6.3.0-235"
                } else if ("$env_classifier" == "hdp2.6.4"){
                    spark_folder = "."
                    hbase_version = "1.1.2.2.6.4.0-91"
                    hadoop_version = "2.7.3.2.6.4.0-91"
                    kafka_version = "0.10.1.2.6.4.0-91"
                    spark_version = "2.2.0.2.6.4.0-91"
                } else if ("$env_classifier" == "hdp2.6.5"){
                    spark_folder = "spark2.3"
                    hbase_version = "1.1.2.2.6.5.0-292"
                    hadoop_version = "2.7.3.2.6.5.0-292"
                    kafka_version = "1.0.0.2.6.5.0-292"
                    spark_version = "2.3.0.2.6.5.0-292"
                } else if ("$env_classifier" == "hdp3.1.0"){
                    spark_folder = "spark2.3"
                    hbase_version = "2.0.2.3.1.0.0-78"
                    hadoop_version = "3.1.1.3.1.0.0-78"
                    kafka_version = "2.0.0.3.1.0.0-78"
                    spark_version = "2.3.2.3.1.0.0-78"
                } else if ("$env_classifier" == "hdp3.1.5"){
                    spark_folder = "spark2.3"
                    hbase_version = "2.1.6.3.1.5.124-1"
                    hadoop_version = "3.1.1.3.1.5.124-1"
                    kafka_version = "2.0.0.3.1.5.124-1"
                    spark_version = "2.3.2.3.1.5.124-1"
                    sh """
                    ls
                    cd ./pipelines/build/
                    cp "hdp3.1.5-assembly.template" ../../$spark_folder/build-assembly.sbt
                    cat build-assembly.sbt
                    """
                }
                sh """
                ls
                cd ./pipelines/build/
                sed -i \"s/REPLACE_SPLICE_VERSION/$splice_version/\" $spark_folder."template"
                sed -i \"s/REPLACE_ENV_CLASSIFIER/$env_classifier/\" $spark_folder."template"
                sed -i \"s/REPLACE_HBASE_VERSION/$hbase_version/\" $spark_folder."template"
                sed -i \"s/REPLACE_HADOOP_VERSION/$hadoop_version/\" $spark_folder."template"
                sed -i \"s/REPLACE_KAFKA_VERSION/$kafka_version/\" $spark_folder."template"
                sed -i \"s/REPLACE_SPARK_VERSION/$spark_version/\" $spark_folder."template"
                cp $spark_folder."template" ../../$spark_folder/build.sbt
                cd ../../$spark_folder/
                cat build.sbt
                ls
                """
                try {
                      sh "cd $spark_folder && sbt clean package"
                      sh "ls && cd $spark_folder/target && ls"
                  } catch (Exception e) {
                      sh "cd $spark_folder && sbt clean package"
                      sh "ls && cd $spark_folder/target && ls"
                  }
                try {
                      sh "cd $spark_folder && sbt assembly"
                      sh "ls && cd $spark_folder/target && ls"
                  } catch (Exception e) {
                      sh "cd $spark_folder && sbt assembly"
                      sh "ls && cd $spark_folder/target && ls"
                  }
                if (release_flag){
                    sh "aws s3 sync $spark_folder/target/scala-*/ s3://splice-releases/${splice_version}/cluster/SSDS/${env_classifier}/ --acl public-read --exclude=\\'*\\' --include=\\'*.jar\\' "
                }
            }
        }
    } catch (any) {
        // if there was an exception thrown, the build failed
        currentBuild.result = "FAILED"
        throw any

    } finally {
        // success or failure, always send notifications
        notifyBuild(currentBuild.result)
    }
}

def notifyBuild(String buildStatus = 'STARTED') {
    // Build status of null means successful.
    buildStatus =  buildStatus ?: 'SUCCESSFUL'
    // Override default values based on build status.
    if (buildStatus == 'STARTED' || buildStatus == 'INPUT') {
        color = 'YELLOW'
        colorCode = '#FFFF00'
    } else if (buildStatus == 'CREATING' || buildStatus == 'DESTROYING'){
        color = 'BLUE'
        colorCode = '#0000FF'
    } else if (buildStatus == 'SUCCESSFUL') {
        color = 'GREEN'
        colorCode = '#00FF00'
    } else if (buildStatus == 'FAILED'){
        color = 'RED'
        colorCode = '#FF0000'
    } else {
        echo "End of pipeline"
    }
}
