# Values for Running Spark Application - Kafka Consumer
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


#Set the docker version of kafka conumer here
image:
  name: jpanko/ssds_dev
  tag:  'spark3.0.1-6'


# Details of the Spark Job to run
job:
  appjar: "/opt/spark/jars/splice-machine-spark-connector_2.12-3.1.0.2002.jar"
  classname: "KafkaReader"

  #Specify the argument to pass to the spark application. The values are space separated with double quotes
  #around values so that has special characters will not result in errors

  # use this for jdbcurl after replacing replace DB_CLUSTER_NAME  DB_USER and DB_PASSWORD
  #"jdbc:splice://splicedb-hregion.DB_CLUSTER_NAME.svc.cluster.local:1527/splicedb;user=DB_USER;password=DB_PASSWORD;"

  #use this for sp;lice kafka, after replacing the DB_CLUSTER_NAME
  #"splicedb-kafka.DB_CLUSTER_NAME.svc.cluster.local:9092"

  #For current application the args required are
  # "JOB_NAME" "EXT_KAFKA_BROKER" "EXT_TOPIC"  "JDBC_URL"  "TABLE_NAME" "SPLICE_KAFKA_BROKER"
#  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092  perf1 jdbc:splice://splicedb-hregion.jp200910.svc.cluster.local:1527/splicedb;user=splice;password=admin; perf1 72 splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092

#  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.jp200910.svc.cluster.local:9092  perf1 jdbc:splice://splicedb-hregion.jp200910.svc.cluster.local:1527/splicedb;user=splice;password=admin; perf1 72 splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.jp200910.svc.cluster.local:9092

#  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.db.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.db.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.db.svc.cluster.local:9092 perf1 .ID.LONG.NOT.NULL,PAYLOAD.STRING,SRC_SERVER.STRING.NOT.NULL,SRC_THREAD.LONG,TM_GENERATED.LONG.NOT.NULL jdbc:splice://splicedb-hregion.db.svc.cluster.local:1527/splicedb;user=splice;password=admin; perf1 splicedb-kafka-0.splicedb-kafka-headless.db.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.db.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.db.svc.cluster.local:9092 16 1 2 true

#  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.db.svc.cluster.local:9092 perf1 .ID.LONG.NOT.NULL,PAYLOAD.STRING,SRC_SERVER.STRING.NOT.NULL,SRC_THREAD.LONG,TM_GENERATED.LONG.NOT.NULL jdbc:splice://splicedb-hregion.db.svc.cluster.local:1527/splicedb;user=splice;password=admin; perf1 splicedb-kafka-0.splicedb-kafka-headless.db.svc.cluster.local:9092 16 1 2 true
  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.db11.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.db11.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.db11.svc.cluster.local:9092 perf1 .ID.LONG.NOT.NULL,PAYLOAD.STRING,SRC_SERVER.STRING.NOT.NULL,SRC_THREAD.LONG,TM_GENERATED.LONG.NOT.NULL jdbc:splice://splicedb-hregion.db11.svc.cluster.local:1527/splicedb;user=splice;password=admin; perf1 splicedb-kafka-0.splicedb-kafka-headless.db11.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.db11.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.db11.svc.cluster.local:9092 16 1 2 true

# External jdbc
#  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.jp200910.svc.cluster.local:9092  perf1 jdbc:splice://jdbc-jp200910-pd-eks-dev2.eks.splicemachine-dev.io:1527/splicedb;ssl=basic;user=splice;password=admin; perf1 72 splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.jp200910.svc.cluster.local:9092  

#splicedb-kafka.jp200826.svc.cluster.local:9092

  appname: "ssds-kafkareader"
  packageslist: org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.kafka:kafka-clients:2.4.1
  #packageslist: org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.kafka:kafka-clients:2.2.1


  # Below are values to run the example pi
  #appjar: "/opt/spark/examples/jars/spark-examples_2.11-2.4.5.jar"
  #classname: "org.apache.spark.examples.SparkPi"
  #appargs: ""
  #appname: "piexample"

  driver_memory: "1g"

  spark_image: jpanko/kafka_spark_dev:spark3.0.1-5
  spark_executor_memory: "4G"
  spark_executor_cores: "5"
  spark_executor_count: "4"

#Set this to true, if you need the pod to not terminate upon finishing the job, so you can log in and debug
debug: true


replicaCount: 1

nameOverride: kafkaconsumer
name: kafkaconsumer
restartPolicy: Never

imagePullPolicy: IfNotPresent

## Annotations passed to operator pod(s).
##
annotations: {}

resources: {}


serviceAccountName: default

nodeselectorLabel: meta







