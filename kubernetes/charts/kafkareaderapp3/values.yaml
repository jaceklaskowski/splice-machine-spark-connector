# Values for Running Spark Application - Kafka Consumer
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


#Set the docker version of kafka conumer here / ssds driver
image:
  name: jpanko/ssds-kafkareaderapp3
  tag:  'pod-spark3.0.1-1'


# Details of the Spark Job to run
job:
  continuousrun: true

  #appjar: "/opt/spark/jars/splice-machine-spark-connector_2.12-3.1.0.2002.jar"
  appjar: "/opt/spark/jars/splice-machine-spark-connector-assembly-3.1.0.2002.jar"
  classname: "com.splicemachine.spark.driver.KafkaReaderApp2"

  #Specify the argument to pass to the spark application. The values are space separated with double quotes
  #around values so that has special characters will not result in errors

  # use this for jdbcurl after replacing replace DB_CLUSTER_NAME  DB_USER and DB_PASSWORD
  #"jdbc:splice://splicedb-hregion.DB_CLUSTER_NAME.svc.cluster.local:1527/splicedb;user=DB_USER;password=DB_PASSWORD;"

  #use this for sp;lice kafka, after replacing the DB_CLUSTER_NAME
  #"splicedb-kafka.DB_CLUSTER_NAME.svc.cluster.local:9092"

  #For current application the args required are
  # "JOB_NAME" "EXT_KAFKA_BROKER" "EXT_TOPIC"  "JDBC_URL"  "TABLE_NAME" "SPLICE_KAFKA_BROKER"
#  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092  perf1 jdbc:splice://splicedb-hregion.jp200910.svc.cluster.local:1527/splicedb;user=splice;password=admin; perf1 72 splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092

#  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.jp200910.svc.cluster.local:9092  perf1 jdbc:splice://splicedb-hregion.jp200910.svc.cluster.local:1527/splicedb;user=splice;password=admin; perf1 72 splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.jp200910.svc.cluster.local:9092

#  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.db.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.db.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.db.svc.cluster.local:9092 perf1 .ID.LONG.NOT.NULL,PAYLOAD.STRING,SRC_SERVER.STRING.NOT.NULL,SRC_THREAD.LONG,TM_GENERATED.LONG.NOT.NULL jdbc:splice://splicedb-hregion.db.svc.cluster.local:1527/splicedb;user=splice;password=admin; perf1 splicedb-kafka-0.splicedb-kafka-headless.db.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.db.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.db.svc.cluster.local:9092 16 1 2 true

#  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.db.svc.cluster.local:9092 perf1 .ID.LONG.NOT.NULL,PAYLOAD.STRING,SRC_SERVER.STRING.NOT.NULL,SRC_THREAD.LONG,TM_GENERATED.LONG.NOT.NULL jdbc:splice://splicedb-hregion.db.svc.cluster.local:1527/splicedb;user=splice;password=admin; perf1 splicedb-kafka-0.splicedb-kafka-headless.db.svc.cluster.local:9092 16 1 2 true

# KafkaReaderApp dev
#  appargs: \"ssds\" splicedb-kafka-0.splicedb-kafka-headless.db1.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.db1.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.db1.svc.cluster.local:9092 sec .VALUE.STRING.NOT.NULL jdbc:splice://splicedb-hregion.db1.svc.cluster.local:1527/splicedb;user=splice;password=admin; sec splicedb-kafka-0.splicedb-kafka-headless.db1.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.db1.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.db1.svc.cluster.local:9092 1 1 2 earliest true true file:/opt/spark/conf/TagNames.csv

# .FULLTAGNAME.STRING.NOT.NULL,TAGNAME.STRING.NOT.NULL,TIME.TIMESTAMP.NOT.NULL,VALUE.DOUBLE.NOT.NULL,QUALITY.INT.NOT.NULL
# .FULLTAGNAME.STRING.NOT.NULL,TAGNAME.STRING.NOT.NULL,TIME.TIMESTAMP.NOT.NULL,VALUE.STRING.NOT.NULL,QUALITY.STRING.NOT.NULL

# IOTDEV03
  appargs: \"kafkareaderapp3\" splicedb-kafka-0.splicedb-kafka-headless.iotdev03.svc.cluster.local:9092 RawTags .TAG.STRING.NOT.NULL,SERVERTIME.TIMESTAMP.NOT.NULL,SOURCETIME.TIMESTAMP.NOT.NULL,VALUE.DOUBLE.NOT.NULL,STATUS.STRING.NOT.NULL jdbc:splice://splicedb-hregion.iotdev03.svc.cluster.local:1527/splicedb;user=splice;password=admin; OCI.RAWTAGS_COMP_PS splicedb-kafka-0.splicedb-kafka-headless.iotdev03.svc.cluster.local:9092 1 1 2 latest hdfs://splicedb-hdfs-nn-0.splicedb-hdfs-nn.iotdev03.svc.cluster.local:8020/tmp;hdfs://splicedb-hdfs-nn-1.splicedb-hdfs-nn.iotdev03.svc.cluster.local:8020/tmp
# Dev k8s
#  appargs: \"kafkareaderapp3\" kafka-broker-0-iotdev03-nonprod-aks-dev1.dev.splicemachine-dev.io:19092 resampling .FULLTAGNAME.STRING.NOT.NULL,TAGNAME.STRING.NOT.NULL,TIME.TIMESTAMP.NOT.NULL,VALUE.DOUBLE.NOT.NULL,QUALITY.INT.NOT.NULL jdbc:splice://splicedb-hregion.db3.svc.cluster.local:1527/splicedb;user=splice;password=admin; oci.resampled_data_1m_stream splicedb-kafka-0.splicedb-kafka-headless.db3.svc.cluster.local:9092 1 1 2 latest true

# External jdbc
#  appargs: \"kafkareader\" splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.jp200910.svc.cluster.local:9092  perf1 jdbc:splice://jdbc-jp200910-pd-eks-dev2.eks.splicemachine-dev.io:1527/splicedb;ssl=basic;user=splice;password=admin; perf1 72 splicedb-kafka-0.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-1.splicedb-kafka-headless.jp200910.svc.cluster.local:9092,splicedb-kafka-2.splicedb-kafka-headless.jp200910.svc.cluster.local:9092  

#splicedb-kafka.jp200826.svc.cluster.local:9092

  appname: "ssds"
  #packageslist: org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.kafka:kafka-clients:2.4.1
  #packageslist: org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.kafka:kafka-clients:2.2.1


  # Below are values to run the example pi
  #appjar: "/opt/spark/examples/jars/spark-examples_2.11-2.4.5.jar"
  #classname: "org.apache.spark.examples.SparkPi"
  #appargs: ""
  #appname: "piexample"

  driver_memory: "1g"

  spark_image: jpanko/ssds-kafkareaderapp3-exec:pod-spark3.0.1-1
  spark_executor_memory: "4G"
  spark_executor_cores: "5"
  spark_executor_count: "4"

#Set this to true, if you need the pod to not terminate upon finishing the job, so you can log in and debug
debug: true


replicaCount: 1

nameOverride: ssds-standalone
name: ssds-standalone
restartPolicy: Never

imagePullPolicy: IfNotPresent

## Annotations passed to operator pod(s).
##
annotations: {}

resources: {}


serviceAccountName: default

nodeselectorLabel: spark







